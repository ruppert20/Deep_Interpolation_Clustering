# Data Format Specification

This document details the complete file structure and data formats required for the Deep Interpolation Clustering pipeline.

## Directory Structure

```
Data/
├── encounter_data/
│   └── encounters.csv              # Patient encounter IDs
├── vital_data/
│   ├── original_data_24h.pickle    # Raw vital signs time series
│   └── next_hour_abnormal_norm_val.csv  # (Optional) Future vital predictions
├── model_data/
│   ├── idx.pickle                  # Train/validation/test split indices
│   ├── checkpoints/                # Intermediate processing checkpoints
│   ├── split_org/                  # Split original data (generated by p0)
│   └── split_processed/            # Processed data ready for training (generated by p0)
└── analysis_data/                  # (Optional) Auxiliary task data
    ├── table_data.csv              # Outcome data (AKI, ICU admission)
    └── mortality_summary.csv       # Mortality outcomes
```

---

## Required Input Files

### 1. Encounter File
**Path:** `Data/encounter_data/encounters.csv`

A CSV file listing all unique patient encounter IDs.

| Column | Type | Description |
|--------|------|-------------|
| `encounter_deiden_id` | string/int | Unique patient encounter identifier |

**Example:**
```csv
encounter_deiden_id
422
1306
7201
172
```

**Notes:**
- IDs can be integers or strings, but must be consistent with vital data
- Each ID should appear only once

---

### 2. Vital Data Pickle
**Path:** `Data/vital_data/original_data_{hours}h.pickle`

A Python dictionary where keys are vital sign names and values are pandas DataFrames.

**Required Keys (configurable in `info.py`):**
```python
{
    'sbp': DataFrame,        # Systolic blood pressure
    'dbp': DataFrame,        # Diastolic blood pressure
    'heartRate': DataFrame,  # Heart rate
    'spo2': DataFrame,       # Oxygen saturation
    'respiratory': DataFrame # Respiratory rate
}
```

**Each DataFrame Schema:**

| Column | Type | Description |
|--------|------|-------------|
| `encounter_deiden_id` | string | Patient encounter ID (must match encounters.csv) |
| `time_stamp` | float | Hours from admission (e.g., 0.0, 0.5, 1.25) |
| `measurement` | float | Vital sign value at that time point |

**Example DataFrame (sbp):**
```
encounter_deiden_id  time_stamp  measurement
422                  0.0         120.0
422                  0.5         118.0
422                  1.2         122.0
1306                 0.0         135.0
1306                 0.8         132.0
```

**Notes:**
- Time series are irregularly sampled
- Each patient may have different numbers of observations
- Missing vitals for a patient are handled by the pipeline
- The `{hours}` in filename should match `--hours_from_admission` parameter

---

### 3. Split Index Pickle
**Path:** `Data/model_data/idx.pickle`

A Python dictionary defining train/validation/test splits.

**Schema:**
```python
{
    'training_idx': List[str],    # List of encounter IDs for training
    'validation_idx': List[str],  # List of encounter IDs for validation
    'testing_idx': List[str]      # List of encounter IDs for testing
}
```

**Example:**
```python
{
    'training_idx': ['422', '1306', '7201', ...],      # ~70% of data
    'validation_idx': ['172', '1497', ...],           # ~15% of data
    'testing_idx': ['890', '2341', ...]               # ~15% of data
}
```

**Notes:**
- IDs must be strings matching those in vital data
- IDs should not overlap between splits
- All IDs should exist in both encounters.csv and vital data

---

## Auxiliary Data Files

These files are required for auxiliary prediction tasks (`--aux_tasks`). The default task is `combined_endpoint`.

### 4. Outcome Table
**Path:** `Data/analysis_data/table_data.csv`

| Column | Type | Description |
|--------|------|-------------|
| `encounter_deiden_id` | string | Patient encounter ID |
| `combined_endpoint` | string | Combined adverse outcome ("Y" or "N") - **default task** |
| `ICU` | string | ICU admission ("Y" or "N") |
| `rapid_response` | string | Rapid response activation ("Y" or "N") |

**Example:**
```csv
encounter_deiden_id,combined_endpoint,ICU,rapid_response
422,N,N,N
1306,Y,N,N
7201,N,Y,Y
```

---

### 5. Mortality Summary
**Path:** `Data/analysis_data/mortality_summary.csv`

| Column | Type | Description |
|--------|------|-------------|
| `encounter_deiden_id` | string | Patient encounter ID |
| `mort_status_30d` | string | In-hospital mortality ("Y" or "N") |
| `icu_mortality` | string | ICU mortality ("Y" or "N") |

**Example:**
```csv
encounter_deiden_id,mort_status_30d,icu_mortality
422,N,N
1306,N,N
7201,Y,Y
```

---

### 6. Future Vital Predictions (Optional)
**Path:** `Data/vital_data/next_hour_abnormal_norm_val.csv`

Only required if using the `future_vital` auxiliary task (not default).

| Column | Type | Description |
|--------|------|-------------|
| `encounter_deiden_id` | string | Patient encounter ID |
| `sbp` | float/null | Next hour SBP abnormality score |
| `dbp` | float/null | Next hour DBP abnormality score |
| `heartRate` | float/null | Next hour heart rate abnormality score |
| `spo2` | float/null | Next hour SpO2 abnormality score |
| `respiratory` | float/null | Next hour respiratory rate abnormality score |

**Notes:**
- Null values are automatically masked during training
- Column names must match `USE_FEATURES` in `info.py`

---

### Available Auxiliary Tasks

| Task | File | Column | Description |
|------|------|--------|-------------|
| `combined_endpoint` | table_data.csv | combined_endpoint | Combined adverse outcome (default) |
| `ICU` | table_data.csv | ICU | ICU admission |
| `rapid_response` | table_data.csv | rapid_response | Rapid response team activation |
| `mort_status_30d` | mortality_summary.csv | mort_status_30d | In-hospital mortality |
| `icu_mortality` | mortality_summary.csv | icu_mortality | ICU mortality |
| `future_vital` | next_hour_abnormal_norm_val.csv | (multiple) | Future vital sign prediction |

### Generating Auxiliary Files from Parquet

If you have an `outcomes_clean.parquet` file, you can generate the CSV files:

```python
import polars as pl
import os

df = pl.read_parquet('Data/outcomes_clean.parquet')
os.makedirs('Data/analysis_data', exist_ok=True)

def int_to_yn(col):
    return pl.when(pl.col(col) == 1).then(pl.lit('Y')).otherwise(pl.lit('N')).alias(col)

# Create table_data.csv
table_data = df.select([
    pl.col('deiden_study_id').cast(pl.Utf8).alias('encounter_deiden_id'),
    int_to_yn('combined_endpoint'),
    pl.when(pl.col('Icu_Admission') == 1).then(pl.lit('Y')).otherwise(pl.lit('N')).alias('ICU'),
    pl.when(pl.col('rapid_response') == 1).then(pl.lit('Y')).otherwise(pl.lit('N')).alias('rapid_response'),
])
table_data.write_csv('Data/analysis_data/table_data.csv')

# Create mortality_summary.csv
mortality_data = df.select([
    pl.col('deiden_study_id').cast(pl.Utf8).alias('encounter_deiden_id'),
    pl.when(pl.col('in_hosp_mortality') == 1).then(pl.lit('Y')).otherwise(pl.lit('N')).alias('mort_status_30d'),
    pl.when(pl.col('icu_mortality') == 1).then(pl.lit('Y')).otherwise(pl.lit('N')).alias('icu_mortality'),
])
mortality_data.write_csv('Data/analysis_data/mortality_summary.csv')
```

---

## Generated Output Files

### From p0_data_process.py

**Split Original Data:** `Data/model_data/split_org/`
```
{cohort}_feat.pickle         # Feature arrays [N, num_features, max_timesteps]
{cohort}_time_step.pickle    # Time stamps
{cohort}_padding_mask.pickle # Observation masks
{cohort}_encounter_id.pickle # Encounter IDs
```

**Processed Data:** `Data/model_data/split_processed/`
```
training.pickle    # Dict with feat, time_step, padding_mask, drop_mask, encounter_id
validation.pickle
testing.pickle
```

---

## Normalization Ranges

The pipeline uses min-max normalization with these default ranges (configurable in `info.py`):

| Vital | Min | Max | Unit |
|-------|-----|-----|------|
| sbp | 20 | 300 | mmHg |
| dbp | 5 | 225 | mmHg |
| heartRate | 0 | 300 | bpm |
| spo2 | 0 | 100 | % |
| respiratory | 0 | 60 | breaths/min |

---

## Configuration File: info.py

Key settings that affect data processing:

```python
# Features to use (must match vital data keys)
USE_FEATURES = ['sbp', 'dbp', 'heartRate', 'spo2', 'respiratory']

# Cohort names
COHORTS = ['training', 'validation', 'testing']

# Normalization ranges
MIN_MAX_VALUES = {
    'sbp': [20, 300],
    'dbp': [5, 225],
    'heartRate': [0, 300],
    'spo2': [0, 100],
    'respiratory': [0, 60]
}
```

---

## Data Type Consistency

**Important:** Ensure encounter IDs are consistent across all files:

| File | ID Column | Recommended Type |
|------|-----------|------------------|
| encounters.csv | encounter_deiden_id | string |
| vital data pickle | encounter_deiden_id | string |
| idx.pickle | *_idx lists | string |
| auxiliary CSVs | encounter_deiden_id | string |

The pipeline converts IDs to strings internally, but consistency prevents issues.
